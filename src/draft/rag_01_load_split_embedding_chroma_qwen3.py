

import sys 
import os
import torch
from pathlib import Path
from dotenv import load_dotenv
import chromadb 

load_dotenv()

__file__ = '/Users/hzz/KMS/IC-RAG-Agent/src/draft/load_split_pdf.py'
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / 'libs/ai-toolkit/'))


def _patch_torch_autocast_for_older_pytorch() -> None:
    """
    On PyTorch < 2.3, torch.is_autocast_enabled() takes no args; transformers
    4.57+ calls torch.is_autocast_enabled(device_type). Patch to accept one
    arg and ignore it, while preserving behavior on newer versions.
    """

    if getattr(torch.is_autocast_enabled, "_qwen_patched", False):
        return

    try:
        # Newer PyTorch supports the device_type argument.
        torch.is_autocast_enabled("cpu")
    except TypeError:
        # Older PyTorch: only no-arg version is available.
        original = torch.is_autocast_enabled

        def _patched(device_type=None):
            return original()

        _patched._qwen_patched = True  # type: ignore[attr-defined]
        torch.is_autocast_enabled = _patched  # type: ignore[assignment]


_patch_torch_autocast_for_older_pytorch()


# 1.load pdf document
from ai_toolkit.rag.loaders import load_pdf_document
pdf_root_path = '/Users/hzz/KMS/IC-RAG-Agent/data/documents/sales_platform/amazon/fba'
pdf_path = '/Users/hzz/KMS/IC-RAG-Agent/data/documents/sales_platform/amazon/fba/FBA features/FBA Global Selling.pdf'
pdf_docs = load_pdf_document(pdf_path)


# 2.split pdf document
from ai_toolkit.rag.splitters import split_document_recursive
split_docs = split_document_recursive(pdf_docs)


# 3.add embedding models - Using local Qwen3-VL-Embedding-2B
from typing import List
from langchain_core.embeddings import Embeddings
from ai_toolkit.rag.retrievers import create_vector_store

# Add model scripts to path
model_scripts_path = project_root / "models/Qwen3-VL-Embedding-2B/scripts"
if model_scripts_path.is_dir():
    sys.path.insert(0, str(model_scripts_path))

from qwen3_vl_embedding import Qwen3VLEmbedder


class LocalQwenEmbeddings(Embeddings):
    """LangChain Embeddings wrapper for local Qwen3-VL-Embedding-2B (text-only)."""

    def __init__(self, model_path: str, max_length: int = 8192):
        self.embedder = Qwen3VLEmbedder(
            model_name_or_path=model_path,
            max_length=max_length,
        )
        print(f"Loaded local Qwen3-VL-Embedding-2B from {model_path}")

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """Embed a list of documents."""
        if not texts:
            return []
        inputs = [{"text": t} for t in texts]
        out = self.embedder.process(inputs, normalize=True)
        # Qwen3-VL may return bfloat16 tensors; convert to float32 for NumPy/langchain.
        out = out.to(dtype=torch.float32)
        arr = out.cpu().numpy()
        if arr.ndim == 1:
            return [arr.tolist()]
        return [arr[i].tolist() for i in range(len(arr))]

    def embed_query(self, text: str) -> List[float]:
        """Embed a single query."""
        inputs = [{"text": text}]
        out = self.embedder.process(inputs, normalize=True)
        # Qwen3-VL may return bfloat16 tensors; convert to float32 for NumPy/langchain.
        out = out.to(dtype=torch.float32)
        arr = out.cpu().numpy()
        if arr.ndim == 2:
            return arr[0].tolist()
        return arr.tolist()


# Initialize local Qwen embeddings and create vector store via ai_toolkit
model_path = str(project_root / "models/Qwen3-VL-Embedding-2B")
embeddings = LocalQwenEmbeddings(model_path)
print("Local Qwen embeddings initialized")

print(f"Creating vector store with {len(split_docs)} document chunks...")
vector_store = create_vector_store(split_docs, embeddings, store_type="inmemory")
print("Vector store created successfully")

# Test similarity search
query = "What is FBA Global Selling?"
print(f"Testing similarity search for: '{query}'")
results = vector_store.similarity_search(query, k=3)
print(f"Found {len(results)} relevant chunks")
if results:
    print("Top result preview:")
    print(f"   {results[0].page_content[:200]}...")
    print(f"   Metadata: {results[0].metadata}")


# ===================== æ–°å¢ï¼šå°†åˆ†å—æ–‡æ¡£å­˜å…¥Chromaå‘é‡åº“ =====================
def store_docs_to_chroma(split_docs, embeddings, chroma_persist_path, collection_name):
    """
    å°†åˆ†å—åçš„æ–‡æ¡£å‘é‡åŒ–å¹¶å­˜å‚¨åˆ°Chroma
    :param split_docs: åˆ†å—åçš„Documentåˆ—è¡¨
    :param embeddings: åˆå§‹åŒ–åçš„LocalQwenEmbeddingså¯¹è±¡
    :param chroma_persist_path: ChromaæŒä¹…åŒ–è·¯å¾„
    :param collection_name: Chromaé›†åˆåç§°
    """
    # 1. åˆå§‹åŒ–ChromaæŒä¹…åŒ–å®¢æˆ·ç«¯ï¼ˆæ•°æ®è½åœ°åˆ°æŒ‡å®šè·¯å¾„ï¼‰
    chroma_client = chromadb.PersistentClient(path=chroma_persist_path)
    # 2. åˆ›å»º/è·å–é›†åˆï¼ˆä¸å­˜åœ¨åˆ™åˆ›å»ºï¼Œå­˜åœ¨åˆ™å¤ç”¨ï¼‰
    collection = chroma_client.get_or_create_collection(name=collection_name)
    
    # 3. æå–æ–‡æ¡£æ ¸å¿ƒä¿¡æ¯
    texts = [doc.page_content for doc in split_docs]  # åˆ†å—æ–‡æœ¬å†…å®¹
    metadatas = [doc.metadata for doc in split_docs]  # å…ƒæ•°æ®ï¼ˆPDFè·¯å¾„ã€é¡µç ç­‰ï¼‰
    ids = [f"{collection_name}_{i}" for i in range(len(split_docs))]  # å”¯ä¸€ID
    
    # 4. ç”Ÿæˆå‘é‡å¹¶æ‰¹é‡å†™å…¥Chroma
    vectors = embeddings.embed_documents(texts)  # è°ƒç”¨Qwenæ¨¡å‹ç”Ÿæˆå‘é‡
    collection.add(
        documents=texts,       # åŸå§‹æ–‡æœ¬ï¼ˆç”¨äºæ£€ç´¢åå±•ç¤ºï¼‰
        metadatas=metadatas,   # æº¯æºå…ƒæ•°æ®
        ids=ids,               # å”¯ä¸€æ ‡è¯†
        embeddings=vectors     # Qwenç”Ÿæˆçš„å‘é‡
    )
    
    print(f"\nâœ… æˆåŠŸå°†{len(texts)}ä¸ªæ–‡æ¡£åˆ†å—å­˜å…¥Chroma")
    print(f"ğŸ“Œ Chromaæ•°æ®æŒä¹…åŒ–è·¯å¾„ï¼š{chroma_persist_path}")
    print(f"ğŸ“Œ Chromaé›†åˆåç§°ï¼š{collection_name}")
    print(f"ğŸ“Œ å‘é‡åº“æ€»æ•°æ®é‡ï¼š{collection.count()}æ¡")

# é…ç½®Chromaå­˜å‚¨å‚æ•°
CHROMA_PERSIST_PATH = str(project_root / "data/chroma_db/amazon/fba")  # æŒ‰ä¸šåŠ¡ç»´åº¦åˆ’åˆ†è·¯å¾„
COLLECTION_NAME = "amazon_fba_features" 

# æ‰§è¡Œå­˜å‚¨æ“ä½œ
store_docs_to_chroma(
    split_docs=split_docs,
    embeddings=embeddings,
    chroma_persist_path=CHROMA_PERSIST_PATH,
    collection_name=COLLECTION_NAME
)


chroma_persist_path = CHROMA_PERSIST_PATH
collection_name = COLLECTION_NAME
query = 'what method is included in the FBA Global Selling?'

# å¯é€‰ï¼šæµ‹è¯•Chromaæ£€ç´¢ï¼ˆéªŒè¯å­˜å‚¨ç»“æœï¼‰
def test_chroma_retrieval(chroma_persist_path, collection_name, query, k=3):
    """Test Chroma vector database retrieval"""
    chroma_client = chromadb.PersistentClient(path=chroma_persist_path)
    collection = chroma_client.get_collection(name=collection_name)
    
    # Generate query vector
    query_vector = embeddings.embed_query(query)
    # Semantic search
    results = collection.query(
        query_embeddings=[query_vector],
        n_results=k,
        include=["documents", "metadatas", "distances"]
    )
    
    print(f"\nğŸ” Chroma retrieval results (query: {query}):")
    for i, (doc, meta, dist) in enumerate(zip(results["documents"][0], results["metadatas"][0], results["distances"][0])):
        print(f"\nResult {i+1} (similarity distance: {dist:.4f}):")
        print(f"Source: {meta.get('source', 'unknown')}, Page: {meta.get('page', 'unknown')}")
        print(f"Content preview: {doc[:200]}...")

# Execute Chroma retrieval test
test_chroma_retrieval(
    chroma_persist_path=CHROMA_PERSIST_PATH,
    collection_name=COLLECTION_NAME,
    query="What is FBA Global Selling?"
)

