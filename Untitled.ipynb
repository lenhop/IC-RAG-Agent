{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4436be90-557d-4a28-a4c4-6c21a52f20d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 'aa'\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "864aefe3-27f9-4c43-aac9-520bc7143399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aa\n"
     ]
    }
   ],
   "source": [
    "a = 'aa'\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6135dbd6-f255-4549-a9aa-fa3e3d66ed1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "__file__ = '/Users/hzz/KMS/IC-RAG-Agent/src/draft/load_split_pdf.py'\n",
    "project_root = Path(__file__).parent.parent.parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "sys.path.insert(0, str(project_root / 'libs/ai-toolkit/'))\n",
    "\n",
    "\n",
    "# 1.load pdf document\n",
    "from ai_toolkit.rag.loaders import load_pdf_document\n",
    "\n",
    "pdf_root_path = '/Users/hzz/KMS/IC-RAG-Agent/data/documents/sales_platform/amazon/fba'\n",
    "\n",
    "pdf_path = '/Users/hzz/KMS/IC-RAG-Agent/data/documents/sales_platform/amazon/fba/FBA features/FBA Global Selling.pdf'\n",
    "\n",
    "pdf_docs = load_pdf_document(pdf_path)\n",
    "# def split_pdf(pdf_path: str):\n",
    "\n",
    "\n",
    "# 2.split pdf document\n",
    "from ai_toolkit.rag.splitters import split_document_recursive\n",
    "\n",
    "split_docs = split_document_recursive(pdf_docs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3d63e5-5d59-4717-b2ac-7a9e0bdcb808",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42438a0d-359d-4a52-9277-762605bf34fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Skia/PDF m144', 'creator': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/144.0.0.0 Safari/537.36', 'creationdate': '2026-02-02T13:37:42+00:00', 'title': 'Amazon', 'moddate': '2026-02-02T13:37:42+00:00', 'source': '/Users/hzz/KMS/IC-RAG-Agent/data/documents/sales_platform/amazon/fba/FBA features/FBA Global Selling.pdf', 'total_pages': 3, 'page': 0, 'page_label': '1'}, page_content='New Seller Central EN Help\\nAdd Products Performance Notifications Payments Fulfillment Edit\\nJuvo + United States Search\\nSearch help articles \\ue02d\\nAll articles\\n\\ue036\\nRecently viewed\\nFEEDBACK\\n2026/2/2 21:37 Amazon\\nhttps://sellercentral.amazon.com/help/hub/reference/201101640 1/3'),\n",
       " Document(metadata={'producer': 'Skia/PDF m144', 'creator': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/144.0.0.0 Safari/537.36', 'creationdate': '2026-02-02T13:37:42+00:00', 'title': 'Amazon', 'moddate': '2026-02-02T13:37:42+00:00', 'source': '/Users/hzz/KMS/IC-RAG-Agent/data/documents/sales_platform/amazon/fba/FBA features/FBA Global Selling.pdf', 'total_pages': 3, 'page': 1, 'page_label': '2'}, page_content='\\ue005 \\ue005\\n\\ue005FBA Global Selling\\nFBA Global Selling\\nOn this page, you will learn how Fulfillment by Amazon can enable you\\nto expand your business globally.\\nThere are two methods of selling globally:\\nFBA Export enables you to list your FBA products on Amazon or your\\nown website and have Amazon export orders to customers in countries\\naround the world.\\nAmazon Global Selling enables you to list and sell your products on any\\none of our websites in the United States, Germany, United Kingdom,\\nFrance, Italy, Spain, Canada, Australia, Singapore, Japan, and the Middle\\nEast. You ship your products to the countries you want to list in and the\\nproducts are fulfilled out of Amazon fulfillment centers in those\\ncountries. You can access various tools and services, including product\\nlisting tools and the Amazon Unified Account.\\nLearn how to setup your Amazon global account\\nWas this article helpful?\\nNeed more help?\\nVisit Seller Forums\\nVisit Seller University\\nGet Support\\nVirtual product bundles\\nSales reports for virtual bundles and virtual multipacks\\nSeller Central Help Get started with Fulfillment by Amazon (FBA)\\nFBA features, services, and fees\\nSummarize this page Ask Seller Assistant\\nYes No\\nFEEDBACK\\n2026/2/2 21:37 Amazon\\nhttps://sellercentral.amazon.com/help/hub/reference/201101640 2/3'),\n",
       " Document(metadata={'producer': 'Skia/PDF m144', 'creator': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/144.0.0.0 Safari/537.36', 'creationdate': '2026-02-02T13:37:42+00:00', 'title': 'Amazon', 'moddate': '2026-02-02T13:37:42+00:00', 'source': '/Users/hzz/KMS/IC-RAG-Agent/data/documents/sales_platform/amazon/fba/FBA features/FBA Global Selling.pdf', 'total_pages': 3, 'page': 2, 'page_label': '3'}, page_content='Â© 1999-2026, Amazon.com, Inc. or its affiliates\\nHelp  Program Policies  English  Download the Amazon Seller mobile app\\nFEEDBACK\\n2026/2/2 21:37 Amazon\\nhttps://sellercentral.amazon.com/help/hub/reference/201101640 3/3')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cb67a6d-b1d4-4422-96d4-8b6c5d739dc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Skia/PDF m144', 'creator': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/144.0.0.0 Safari/537.36', 'creationdate': '2026-02-02T13:37:42+00:00', 'title': 'Amazon', 'moddate': '2026-02-02T13:37:42+00:00', 'source': '/Users/hzz/KMS/IC-RAG-Agent/data/documents/sales_platform/amazon/fba/FBA features/FBA Global Selling.pdf', 'total_pages': 3, 'page': 0, 'page_label': '1', 'start_index': 0}, page_content='New Seller Central EN Help\\nAdd Products Performance Notifications Payments Fulfillment Edit\\nJuvo + United States Search\\nSearch help articles \\ue02d\\nAll articles\\n\\ue036\\nRecently viewed\\nFEEDBACK\\n2026/2/2 21:37 Amazon\\nhttps://sellercentral.amazon.com/help/hub/reference/201101640 1/3'),\n",
       " Document(metadata={'producer': 'Skia/PDF m144', 'creator': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/144.0.0.0 Safari/537.36', 'creationdate': '2026-02-02T13:37:42+00:00', 'title': 'Amazon', 'moddate': '2026-02-02T13:37:42+00:00', 'source': '/Users/hzz/KMS/IC-RAG-Agent/data/documents/sales_platform/amazon/fba/FBA features/FBA Global Selling.pdf', 'total_pages': 3, 'page': 1, 'page_label': '2', 'start_index': 0}, page_content='\\ue005 \\ue005\\n\\ue005FBA Global Selling\\nFBA Global Selling\\nOn this page, you will learn how Fulfillment by Amazon can enable you\\nto expand your business globally.\\nThere are two methods of selling globally:\\nFBA Export enables you to list your FBA products on Amazon or your\\nown website and have Amazon export orders to customers in countries\\naround the world.\\nAmazon Global Selling enables you to list and sell your products on any\\none of our websites in the United States, Germany, United Kingdom,\\nFrance, Italy, Spain, Canada, Australia, Singapore, Japan, and the Middle\\nEast. You ship your products to the countries you want to list in and the\\nproducts are fulfilled out of Amazon fulfillment centers in those\\ncountries. You can access various tools and services, including product\\nlisting tools and the Amazon Unified Account.\\nLearn how to setup your Amazon global account\\nWas this article helpful?\\nNeed more help?\\nVisit Seller Forums\\nVisit Seller University\\nGet Support\\nVirtual product bundles'),\n",
       " Document(metadata={'producer': 'Skia/PDF m144', 'creator': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/144.0.0.0 Safari/537.36', 'creationdate': '2026-02-02T13:37:42+00:00', 'title': 'Amazon', 'moddate': '2026-02-02T13:37:42+00:00', 'source': '/Users/hzz/KMS/IC-RAG-Agent/data/documents/sales_platform/amazon/fba/FBA features/FBA Global Selling.pdf', 'total_pages': 3, 'page': 1, 'page_label': '2', 'start_index': 814}, page_content='Learn how to setup your Amazon global account\\nWas this article helpful?\\nNeed more help?\\nVisit Seller Forums\\nVisit Seller University\\nGet Support\\nVirtual product bundles\\nSales reports for virtual bundles and virtual multipacks\\nSeller Central Help Get started with Fulfillment by Amazon (FBA)\\nFBA features, services, and fees\\nSummarize this page Ask Seller Assistant\\nYes No\\nFEEDBACK\\n2026/2/2 21:37 Amazon\\nhttps://sellercentral.amazon.com/help/hub/reference/201101640 2/3'),\n",
       " Document(metadata={'producer': 'Skia/PDF m144', 'creator': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/144.0.0.0 Safari/537.36', 'creationdate': '2026-02-02T13:37:42+00:00', 'title': 'Amazon', 'moddate': '2026-02-02T13:37:42+00:00', 'source': '/Users/hzz/KMS/IC-RAG-Agent/data/documents/sales_platform/amazon/fba/FBA features/FBA Global Selling.pdf', 'total_pages': 3, 'page': 2, 'page_label': '3', 'start_index': 0}, page_content='Â© 1999-2026, Amazon.com, Inc. or its affiliates\\nHelp  Program Policies  English  Download the Amazon Seller mobile app\\nFEEDBACK\\n2026/2/2 21:37 Amazon\\nhttps://sellercentral.amazon.com/help/hub/reference/201101640 3/3')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ecfb4f42-d440-4225-b736-98a57c62b2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import List\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from langchain_core.vectorstores import InMemoryVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d713c5d9-cc7b-454e-a892-03adfe4460db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add model scripts to path\n",
    "model_scripts_path = project_root / 'models/Qwen3-VL-Embedding-8B/scripts'\n",
    "sys.path.insert(0, str(model_scripts_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3fe307f2-b46d-4351-aa36-b282bfde7f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qwen3_vl_embedding import Qwen3VLEmbedder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc03ba61-ef10-4781-9731-22cbbdc34d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Custom LangChain Embeddings wrapper for local Qwen model\n",
    "class LocalQwenEmbeddings(Embeddings):\n",
    "    def __init__(self, model_path: str):\n",
    "        self.embedder = Qwen3VLEmbedder(\n",
    "            model_name_or_path=model_path,\n",
    "            max_length=8192\n",
    "        )\n",
    "        print(f\"âœ“ Loaded local Qwen3-VL-Embedding-8B from {model_path}\")\n",
    "    \n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Embed a list of documents.\"\"\"\n",
    "        inputs = [{'text': text} for text in texts]\n",
    "        embeddings = self.embedder.process(inputs, normalize=True)\n",
    "        return embeddings.cpu().numpy().tolist()\n",
    "    \n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        \"\"\"Embed a single query.\"\"\"\n",
    "        inputs = [{'text': text}]\n",
    "        embedding = self.embedder.process(inputs, normalize=True)\n",
    "        return embedding.cpu().numpy().tolist()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9786023a-d9b1-4c8a-9c46-26871d33ef52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c264aada25a4bcc9b44662823386bcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/749 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Loaded local Qwen3-VL-Embedding-8B from /Users/hzz/KMS/IC-RAG-Agent/models/Qwen3-VL-Embedding-8B\n",
      "âœ“ Local Qwen embeddings initialized\n"
     ]
    }
   ],
   "source": [
    "# Initialize local Qwen embeddings\n",
    "model_path = str(project_root / 'models/Qwen3-VL-Embedding-8B')\n",
    "embeddings = LocalQwenEmbeddings(model_path)\n",
    "print(\"âœ“ Local Qwen embeddings initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "179d94c2-c53e-4c8d-a766-580736d2629c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Creating vector store with 4 document chunks...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "torch.is_autocast_enabled() takes no arguments (1 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.11/site-packages/transformers/utils/generic.py:1009\u001b[39m, in \u001b[36mcheck_model_inputs.<locals>.wrapped_fn.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1008\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1009\u001b[39m     outputs = func(\u001b[38;5;28mself\u001b[39m, *args, **kwargs_without_recordable)\n\u001b[32m   1010\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.11/site-packages/transformers/models/qwen3_vl/modeling_qwen3_vl.py:907\u001b[39m, in \u001b[36mQwen3VLTextModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, cache_position, visual_pos_masks, deepstack_visual_embeds, **kwargs)\u001b[39m\n\u001b[32m    906\u001b[39m \u001b[38;5;66;03m# create position embeddings to be shared across the decoder layers\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m907\u001b[39m position_embeddings = \u001b[38;5;28mself\u001b[39m.rotary_emb(hidden_states, position_ids)\n\u001b[32m    909\u001b[39m \u001b[38;5;66;03m# decoder layers\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.11/site-packages/transformers/modeling_rope_utils.py:127\u001b[39m, in \u001b[36mdynamic_rope_update.<locals>.wrapper\u001b[39m\u001b[34m(self, x, position_ids, layer_type)\u001b[39m\n\u001b[32m    126\u001b[39m     longrope_frequency_update(\u001b[38;5;28mself\u001b[39m, position_ids, device=x.device, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m rope_forward(\u001b[38;5;28mself\u001b[39m, x, position_ids, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.11/site-packages/transformers/models/qwen3_vl/modeling_qwen3_vl.py:353\u001b[39m, in \u001b[36mQwen3VLTextRotaryEmbedding.forward\u001b[39m\u001b[34m(self, x, position_ids)\u001b[39m\n\u001b[32m    352\u001b[39m device_type = x.device.type \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x.device.type, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m x.device.type != \u001b[33m\"\u001b[39m\u001b[33mmps\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m maybe_autocast(device_type=device_type, enabled=\u001b[38;5;28;01mFalse\u001b[39;00m):  \u001b[38;5;66;03m# Force float32\u001b[39;00m\n\u001b[32m    354\u001b[39m     freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(\u001b[32m2\u001b[39m, \u001b[32m3\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.11/site-packages/transformers/utils/generic.py:194\u001b[39m, in \u001b[36mmaybe_autocast\u001b[39m\u001b[34m(device_type, dtype, enabled, cache_enabled)\u001b[39m\n\u001b[32m    184\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    185\u001b[39m \u001b[33;03mContext manager that only autocasts if:\u001b[39;00m\n\u001b[32m    186\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    192\u001b[39m \u001b[33;03mrequirement that partition IDs be monotonically increasing.\u001b[39;00m\n\u001b[32m    193\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.is_autocast_enabled(device_type) \u001b[38;5;129;01mor\u001b[39;00m enabled:\n\u001b[32m    195\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.autocast(device_type, dtype=dtype, enabled=enabled, cache_enabled=cache_enabled)\n",
      "\u001b[31mTypeError\u001b[39m: torch.is_autocast_enabled() takes no arguments (1 given)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.11/site-packages/transformers/utils/generic.py:1009\u001b[39m, in \u001b[36mcheck_model_inputs.<locals>.wrapped_fn.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1008\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1009\u001b[39m     outputs = func(\u001b[38;5;28mself\u001b[39m, *args, **kwargs_without_recordable)\n\u001b[32m   1010\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.11/site-packages/transformers/models/qwen3_vl/modeling_qwen3_vl.py:1274\u001b[39m, in \u001b[36mQwen3VLModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, pixel_values, pixel_values_videos, image_grid_thw, video_grid_thw, cache_position, **kwargs)\u001b[39m\n\u001b[32m   1272\u001b[39m         position_ids = position_ids.unsqueeze(\u001b[32m0\u001b[39m).expand(\u001b[32m3\u001b[39m, -\u001b[32m1\u001b[39m, -\u001b[32m1\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1274\u001b[39m outputs = \u001b[38;5;28mself\u001b[39m.language_model(\n\u001b[32m   1275\u001b[39m     input_ids=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1276\u001b[39m     position_ids=position_ids,\n\u001b[32m   1277\u001b[39m     attention_mask=attention_mask,\n\u001b[32m   1278\u001b[39m     past_key_values=past_key_values,\n\u001b[32m   1279\u001b[39m     inputs_embeds=inputs_embeds,\n\u001b[32m   1280\u001b[39m     cache_position=cache_position,\n\u001b[32m   1281\u001b[39m     visual_pos_masks=visual_pos_masks,\n\u001b[32m   1282\u001b[39m     deepstack_visual_embeds=deepstack_visual_embeds,\n\u001b[32m   1283\u001b[39m     **kwargs,\n\u001b[32m   1284\u001b[39m )\n\u001b[32m   1286\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Qwen3VLModelOutputWithPast(\n\u001b[32m   1287\u001b[39m     last_hidden_state=outputs.last_hidden_state,\n\u001b[32m   1288\u001b[39m     past_key_values=outputs.past_key_values,\n\u001b[32m   1289\u001b[39m     rope_deltas=\u001b[38;5;28mself\u001b[39m.rope_deltas,\n\u001b[32m   1290\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.11/site-packages/transformers/utils/generic.py:1011\u001b[39m, in \u001b[36mcheck_model_inputs.<locals>.wrapped_fn.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1010\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1011\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m original_exception\n\u001b[32m   1012\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m   1013\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mMissing `**kwargs` in the signature of the `@check_model_inputs`-decorated function \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1014\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1015\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.11/site-packages/transformers/utils/generic.py:1002\u001b[39m, in \u001b[36mcheck_model_inputs.<locals>.wrapped_fn.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1001\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1002\u001b[39m         outputs = func(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n\u001b[32m   1003\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m original_exception:\n\u001b[32m   1004\u001b[39m     \u001b[38;5;66;03m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[39;00m\n\u001b[32m   1005\u001b[39m     \u001b[38;5;66;03m# Get a TypeError even after removing the recordable kwargs -> re-raise the original exception\u001b[39;00m\n\u001b[32m   1006\u001b[39m     \u001b[38;5;66;03m# Otherwise -> we're probably missing `**kwargs` in the decorated function\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.11/site-packages/transformers/models/qwen3_vl/modeling_qwen3_vl.py:907\u001b[39m, in \u001b[36mQwen3VLTextModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, cache_position, visual_pos_masks, deepstack_visual_embeds, **kwargs)\u001b[39m\n\u001b[32m    906\u001b[39m \u001b[38;5;66;03m# create position embeddings to be shared across the decoder layers\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m907\u001b[39m position_embeddings = \u001b[38;5;28mself\u001b[39m.rotary_emb(hidden_states, position_ids)\n\u001b[32m    909\u001b[39m \u001b[38;5;66;03m# decoder layers\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.11/site-packages/transformers/modeling_rope_utils.py:127\u001b[39m, in \u001b[36mdynamic_rope_update.<locals>.wrapper\u001b[39m\u001b[34m(self, x, position_ids, layer_type)\u001b[39m\n\u001b[32m    126\u001b[39m     longrope_frequency_update(\u001b[38;5;28mself\u001b[39m, position_ids, device=x.device, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m rope_forward(\u001b[38;5;28mself\u001b[39m, x, position_ids, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.11/site-packages/transformers/models/qwen3_vl/modeling_qwen3_vl.py:353\u001b[39m, in \u001b[36mQwen3VLTextRotaryEmbedding.forward\u001b[39m\u001b[34m(self, x, position_ids)\u001b[39m\n\u001b[32m    352\u001b[39m device_type = x.device.type \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x.device.type, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m x.device.type != \u001b[33m\"\u001b[39m\u001b[33mmps\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m maybe_autocast(device_type=device_type, enabled=\u001b[38;5;28;01mFalse\u001b[39;00m):  \u001b[38;5;66;03m# Force float32\u001b[39;00m\n\u001b[32m    354\u001b[39m     freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(\u001b[32m2\u001b[39m, \u001b[32m3\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.11/site-packages/transformers/utils/generic.py:194\u001b[39m, in \u001b[36mmaybe_autocast\u001b[39m\u001b[34m(device_type, dtype, enabled, cache_enabled)\u001b[39m\n\u001b[32m    184\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    185\u001b[39m \u001b[33;03mContext manager that only autocasts if:\u001b[39;00m\n\u001b[32m    186\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    192\u001b[39m \u001b[33;03mrequirement that partition IDs be monotonically increasing.\u001b[39;00m\n\u001b[32m    193\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.is_autocast_enabled(device_type) \u001b[38;5;129;01mor\u001b[39;00m enabled:\n\u001b[32m    195\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.autocast(device_type, dtype=dtype, enabled=enabled, cache_enabled=cache_enabled)\n",
      "\u001b[31mTypeError\u001b[39m: torch.is_autocast_enabled() takes no arguments (1 given)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mðŸ“Š Creating vector store with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(split_docs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m document chunks...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m vector_store = InMemoryVectorStore(embeddings)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m vector_store.add_documents(split_docs)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mâœ“ Vector store created successfully\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.11/site-packages/langchain_core/vectorstores/in_memory.py:195\u001b[39m, in \u001b[36mInMemoryVectorStore.add_documents\u001b[39m\u001b[34m(self, documents, ids, **kwargs)\u001b[39m\n\u001b[32m    187\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34madd_documents\u001b[39m(\n\u001b[32m    189\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    192\u001b[39m     **kwargs: Any,\n\u001b[32m    193\u001b[39m ) -> \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    194\u001b[39m     texts = [doc.page_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[32m--> \u001b[39m\u001b[32m195\u001b[39m     vectors = \u001b[38;5;28mself\u001b[39m.embedding.embed_documents(texts)\n\u001b[32m    197\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ids \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(ids) != \u001b[38;5;28mlen\u001b[39m(texts):\n\u001b[32m    198\u001b[39m         msg = (\n\u001b[32m    199\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mids must be the same length as texts. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    200\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(ids)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ids and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(texts)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m texts.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    201\u001b[39m         )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mLocalQwenEmbeddings.embed_documents\u001b[39m\u001b[34m(self, texts)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Embed a list of documents.\"\"\"\u001b[39;00m\n\u001b[32m     12\u001b[39m inputs = [{\u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m: text} \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts]\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m embeddings = \u001b[38;5;28mself\u001b[39m.embedder.process(inputs, normalize=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings.cpu().numpy().tolist()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/KMS/IC-RAG-Agent/models/Qwen3-VL-Embedding-8B/scripts/qwen3_vl_embedding.py:330\u001b[39m, in \u001b[36mQwen3VLEmbedder.process\u001b[39m\u001b[34m(self, inputs, normalize)\u001b[39m\n\u001b[32m    327\u001b[39m processed_inputs = \u001b[38;5;28mself\u001b[39m._preprocess_inputs(conversations)\n\u001b[32m    328\u001b[39m processed_inputs = {k: v.to(\u001b[38;5;28mself\u001b[39m.model.device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m processed_inputs.items()}\n\u001b[32m--> \u001b[39m\u001b[32m330\u001b[39m outputs = \u001b[38;5;28mself\u001b[39m.forward(processed_inputs)\n\u001b[32m    331\u001b[39m embeddings = \u001b[38;5;28mself\u001b[39m._pooling_last(outputs[\u001b[33m'\u001b[39m\u001b[33mlast_hidden_state\u001b[39m\u001b[33m'\u001b[39m], outputs[\u001b[33m'\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m    333\u001b[39m \u001b[38;5;66;03m# Normalize the embeddings if specified\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/KMS/IC-RAG-Agent/models/Qwen3-VL-Embedding-8B/scripts/qwen3_vl_embedding.py:172\u001b[39m, in \u001b[36mQwen3VLEmbedder.forward\u001b[39m\u001b[34m(self, inputs)\u001b[39m\n\u001b[32m    170\u001b[39m \u001b[38;5;129m@torch\u001b[39m.no_grad()\n\u001b[32m    171\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any]) -> Dict[\u001b[38;5;28mstr\u001b[39m, torch.Tensor]:\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m     outputs = \u001b[38;5;28mself\u001b[39m.model(**inputs)\n\u001b[32m    173\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m    174\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mlast_hidden_state\u001b[39m\u001b[33m'\u001b[39m: outputs.last_hidden_state,\n\u001b[32m    175\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m'\u001b[39m: inputs.get(\u001b[33m'\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    176\u001b[39m     }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/KMS/IC-RAG-Agent/models/Qwen3-VL-Embedding-8B/scripts/qwen3_vl_embedding.py:98\u001b[39m, in \u001b[36mQwen3VLForEmbedding.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, pixel_values, pixel_values_videos, image_grid_thw, video_grid_thw, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[32m     84\u001b[39m             input_ids: torch.LongTensor = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     85\u001b[39m             attention_mask: Optional[torch.Tensor] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     96\u001b[39m ) -> Union[\u001b[38;5;28mtuple\u001b[39m, Qwen3VLForEmbeddingOutput]:\n\u001b[32m     97\u001b[39m     \u001b[38;5;66;03m# Pass inputs through the model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m     outputs = \u001b[38;5;28mself\u001b[39m.model(\n\u001b[32m     99\u001b[39m         input_ids=input_ids,\n\u001b[32m    100\u001b[39m         pixel_values=pixel_values,\n\u001b[32m    101\u001b[39m         pixel_values_videos=pixel_values_videos,\n\u001b[32m    102\u001b[39m         image_grid_thw=image_grid_thw,\n\u001b[32m    103\u001b[39m         video_grid_thw=video_grid_thw,\n\u001b[32m    104\u001b[39m         position_ids=position_ids,\n\u001b[32m    105\u001b[39m         attention_mask=attention_mask,\n\u001b[32m    106\u001b[39m         past_key_values=past_key_values,\n\u001b[32m    107\u001b[39m         inputs_embeds=inputs_embeds,\n\u001b[32m    108\u001b[39m         cache_position=cache_position,\n\u001b[32m    109\u001b[39m         **kwargs,\n\u001b[32m    110\u001b[39m     )\n\u001b[32m    111\u001b[39m     \u001b[38;5;66;03m# Return the model output\u001b[39;00m\n\u001b[32m    112\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m Qwen3VLForEmbeddingOutput(\n\u001b[32m    113\u001b[39m         last_hidden_state=outputs.last_hidden_state,\n\u001b[32m    114\u001b[39m         attention_mask=attention_mask,\n\u001b[32m    115\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.11/site-packages/transformers/utils/generic.py:1011\u001b[39m, in \u001b[36mcheck_model_inputs.<locals>.wrapped_fn.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1009\u001b[39m         outputs = func(\u001b[38;5;28mself\u001b[39m, *args, **kwargs_without_recordable)\n\u001b[32m   1010\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1011\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m original_exception\n\u001b[32m   1012\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m   1013\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mMissing `**kwargs` in the signature of the `@check_model_inputs`-decorated function \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1014\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1015\u001b[39m     )\n\u001b[32m   1017\u001b[39m \u001b[38;5;66;03m# Restore original forward methods\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.11/site-packages/transformers/utils/generic.py:1002\u001b[39m, in \u001b[36mcheck_model_inputs.<locals>.wrapped_fn.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1000\u001b[39m             outputs = func(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n\u001b[32m   1001\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1002\u001b[39m         outputs = func(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n\u001b[32m   1003\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m original_exception:\n\u001b[32m   1004\u001b[39m     \u001b[38;5;66;03m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[39;00m\n\u001b[32m   1005\u001b[39m     \u001b[38;5;66;03m# Get a TypeError even after removing the recordable kwargs -> re-raise the original exception\u001b[39;00m\n\u001b[32m   1006\u001b[39m     \u001b[38;5;66;03m# Otherwise -> we're probably missing `**kwargs` in the decorated function\u001b[39;00m\n\u001b[32m   1007\u001b[39m     kwargs_without_recordable = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m recordable_keys}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.11/site-packages/transformers/models/qwen3_vl/modeling_qwen3_vl.py:1274\u001b[39m, in \u001b[36mQwen3VLModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, pixel_values, pixel_values_videos, image_grid_thw, video_grid_thw, cache_position, **kwargs)\u001b[39m\n\u001b[32m   1271\u001b[39m         position_ids = position_ids.add(delta)\n\u001b[32m   1272\u001b[39m         position_ids = position_ids.unsqueeze(\u001b[32m0\u001b[39m).expand(\u001b[32m3\u001b[39m, -\u001b[32m1\u001b[39m, -\u001b[32m1\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1274\u001b[39m outputs = \u001b[38;5;28mself\u001b[39m.language_model(\n\u001b[32m   1275\u001b[39m     input_ids=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1276\u001b[39m     position_ids=position_ids,\n\u001b[32m   1277\u001b[39m     attention_mask=attention_mask,\n\u001b[32m   1278\u001b[39m     past_key_values=past_key_values,\n\u001b[32m   1279\u001b[39m     inputs_embeds=inputs_embeds,\n\u001b[32m   1280\u001b[39m     cache_position=cache_position,\n\u001b[32m   1281\u001b[39m     visual_pos_masks=visual_pos_masks,\n\u001b[32m   1282\u001b[39m     deepstack_visual_embeds=deepstack_visual_embeds,\n\u001b[32m   1283\u001b[39m     **kwargs,\n\u001b[32m   1284\u001b[39m )\n\u001b[32m   1286\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Qwen3VLModelOutputWithPast(\n\u001b[32m   1287\u001b[39m     last_hidden_state=outputs.last_hidden_state,\n\u001b[32m   1288\u001b[39m     past_key_values=outputs.past_key_values,\n\u001b[32m   1289\u001b[39m     rope_deltas=\u001b[38;5;28mself\u001b[39m.rope_deltas,\n\u001b[32m   1290\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.11/site-packages/transformers/utils/generic.py:1011\u001b[39m, in \u001b[36mcheck_model_inputs.<locals>.wrapped_fn.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1009\u001b[39m         outputs = func(\u001b[38;5;28mself\u001b[39m, *args, **kwargs_without_recordable)\n\u001b[32m   1010\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1011\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m original_exception\n\u001b[32m   1012\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m   1013\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mMissing `**kwargs` in the signature of the `@check_model_inputs`-decorated function \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1014\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1015\u001b[39m     )\n\u001b[32m   1017\u001b[39m \u001b[38;5;66;03m# Restore original forward methods\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.11/site-packages/transformers/utils/generic.py:1002\u001b[39m, in \u001b[36mcheck_model_inputs.<locals>.wrapped_fn.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1000\u001b[39m             outputs = func(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n\u001b[32m   1001\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1002\u001b[39m         outputs = func(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n\u001b[32m   1003\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m original_exception:\n\u001b[32m   1004\u001b[39m     \u001b[38;5;66;03m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[39;00m\n\u001b[32m   1005\u001b[39m     \u001b[38;5;66;03m# Get a TypeError even after removing the recordable kwargs -> re-raise the original exception\u001b[39;00m\n\u001b[32m   1006\u001b[39m     \u001b[38;5;66;03m# Otherwise -> we're probably missing `**kwargs` in the decorated function\u001b[39;00m\n\u001b[32m   1007\u001b[39m     kwargs_without_recordable = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m recordable_keys}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.11/site-packages/transformers/models/qwen3_vl/modeling_qwen3_vl.py:907\u001b[39m, in \u001b[36mQwen3VLTextModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, cache_position, visual_pos_masks, deepstack_visual_embeds, **kwargs)\u001b[39m\n\u001b[32m    904\u001b[39m hidden_states = inputs_embeds\n\u001b[32m    906\u001b[39m \u001b[38;5;66;03m# create position embeddings to be shared across the decoder layers\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m907\u001b[39m position_embeddings = \u001b[38;5;28mself\u001b[39m.rotary_emb(hidden_states, position_ids)\n\u001b[32m    909\u001b[39m \u001b[38;5;66;03m# decoder layers\u001b[39;00m\n\u001b[32m    910\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer_idx, decoder_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m.layers):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.11/site-packages/transformers/modeling_rope_utils.py:127\u001b[39m, in \u001b[36mdynamic_rope_update.<locals>.wrapper\u001b[39m\u001b[34m(self, x, position_ids, layer_type)\u001b[39m\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m rope_type == \u001b[33m\"\u001b[39m\u001b[33mlongrope\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    126\u001b[39m     longrope_frequency_update(\u001b[38;5;28mself\u001b[39m, position_ids, device=x.device, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m rope_forward(\u001b[38;5;28mself\u001b[39m, x, position_ids, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.11/site-packages/transformers/models/qwen3_vl/modeling_qwen3_vl.py:353\u001b[39m, in \u001b[36mQwen3VLTextRotaryEmbedding.forward\u001b[39m\u001b[34m(self, x, position_ids)\u001b[39m\n\u001b[32m    350\u001b[39m position_ids_expanded = position_ids[:, :, \u001b[38;5;28;01mNone\u001b[39;00m, :].float()  \u001b[38;5;66;03m# shape (3, bs, 1, positions)\u001b[39;00m\n\u001b[32m    352\u001b[39m device_type = x.device.type \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x.device.type, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m x.device.type != \u001b[33m\"\u001b[39m\u001b[33mmps\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m maybe_autocast(device_type=device_type, enabled=\u001b[38;5;28;01mFalse\u001b[39;00m):  \u001b[38;5;66;03m# Force float32\u001b[39;00m\n\u001b[32m    354\u001b[39m     freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(\u001b[32m2\u001b[39m, \u001b[32m3\u001b[39m)\n\u001b[32m    355\u001b[39m     freqs = \u001b[38;5;28mself\u001b[39m.apply_interleaved_mrope(freqs, \u001b[38;5;28mself\u001b[39m.mrope_section)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.11/site-packages/transformers/utils/generic.py:194\u001b[39m, in \u001b[36mmaybe_autocast\u001b[39m\u001b[34m(device_type, dtype, enabled, cache_enabled)\u001b[39m\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmaybe_autocast\u001b[39m(\n\u001b[32m    179\u001b[39m     device_type: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    180\u001b[39m     dtype: Optional[\u001b[33m\"\u001b[39m\u001b[33m_dtype\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    181\u001b[39m     enabled: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    182\u001b[39m     cache_enabled: \u001b[38;5;28mbool\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    183\u001b[39m ):\n\u001b[32m    184\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    185\u001b[39m \u001b[33;03m    Context manager that only autocasts if:\u001b[39;00m\n\u001b[32m    186\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    192\u001b[39m \u001b[33;03m    requirement that partition IDs be monotonically increasing.\u001b[39;00m\n\u001b[32m    193\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch.is_autocast_enabled(device_type) \u001b[38;5;129;01mor\u001b[39;00m enabled:\n\u001b[32m    195\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m torch.autocast(device_type, dtype=dtype, enabled=enabled, cache_enabled=cache_enabled)\n\u001b[32m    196\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mTypeError\u001b[39m: torch.is_autocast_enabled() takes no arguments (1 given)"
     ]
    }
   ],
   "source": [
    "# Create vector store and add documents\n",
    "print(f\"\\nðŸ“Š Creating vector store with {len(split_docs)} document chunks...\")\n",
    "vector_store = InMemoryVectorStore(embeddings)\n",
    "vector_store.add_documents(split_docs)\n",
    "print(f\"âœ“ Vector store created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ace8118-998e-4682-96b3-6212da4ad8e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
